{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement\n",
        "\n",
        "In order to develop any drug, the first step is to do drug discovery. Now, the question arrises is that what is Drug Discovery ? Well guys, Drug Discovery is about finding valid drug molecules, refining them and selecting the good ones through some kind of algorithm, so that the best selected molecule can be used to manufacture drugs to treat diseases.\n",
        "\n",
        "Now the problem is that there are almost $10^{60}$ drug molecules synthetically available, from which we have to first select valid molecules and do further preprocessing on them to select the good ones to develop drugs. Now, $10^{60}$ is a very huge number of molecules to search from and it is going to take lot of time to search the valid molecules. \n",
        "\n",
        "# What we have to do in this project ? \n",
        "\n",
        "In order to solve this problem, a new methodology called de novo drug design is proposed. In this methodology, molecules are generated from scratch using several generative algorithms in Deep Learning such as GANs or Sequence to Sequence Recurrent Neural Networks. These generated molecules will be valid and vastly different from those that were used as training data to train these neural networks. And that is what we will be doing in this project. \n",
        "\n",
        "# Step 1:\n",
        "We will be training a Sequence to Sequence Recurrent Neural Network (RNN) to generate synthetic (fake, real looking and valid) molecules. \n",
        "\n",
        "# Step 2:\n",
        "Furthermore, we will be selecting the good molecules from the valid molecules generated by Seq to Seq RNN, through some kind of an algorithm which is based on the concepts on biotechnology which is not going to be our focus (it's code will be already provided so that we don't have to think about implementing this algorithm) and we will skip implementing it's code. \n",
        "\n",
        "# Step 3:\n",
        "Once, we get good molecules, then we use these good molecules to further fine tune the parameters of our Sequence to Sequence RNN so that it starts to generate not only valid but good molecules.\n",
        "\n",
        "We have to keep running these 3 steps in a loop, until the Sequence to Sequence RNN starts to generate good molecules. \n",
        "\n",
        "# Training Data\n",
        "Another question which arrises is that from where we will be getting our training data to train our Sequence to Sequence RNN. So, first of all the training dataset of 500,000 molecules is taken from the open-source ChEMBL dataset of drug-like molecules which is curated by the European Bioinformatics Institute. You can read more about this dataset here:\n",
        "https://www.ebi.ac.uk/chembl/\n",
        "\n",
        "In this dataset, the molecules were represented using SMILES (Simplified Molecular Input Line-Entry System). Using SMILES, molecules are encoded as strings. Molecules were represented using the SMILES string notation for easy interpretation by the recurrent neural network model we employ. SMILES was specifically designed with grammatical consistency and machine friendliness in mind, using characters to represent atoms, bonds, and chemical structures. For example, aromatic and aliphatic carbon atoms are represented by the symbols c and C. Single, double, and triple bonds are represented by the characters -, = ,\n",
        "\\#, respectively. Parenthese enclosures are used to show branches, and rings are indicated by digits immediately following the atoms where the ring is closed. The 500,000 molecules collected totalled 25 million SMILES characters. Additionally, start and end characters of “G” (go) and “\\n” (new line) were appended to each molecule, yielding a total vocabulary of 53 unique characters within the dataset. All molecules were between 35 and 75 characters in length.\n",
        "\n",
        "# Some Example Molecules and their SMILES string representations\n",
        "\n",
        "<img src = 'https://drive.google.com/uc?id=150mi8DtiB5J1FsULIHd1Hji0EmfVM4do'>\n",
        "\n",
        "The above picture shows SMILES string representation of two very popular drug molecules. Let's have a look on the SMILES string representation of a Drug Molecule named Epinephrine: \n",
        "\n",
        "# $CNC[C@H](O)c1ccc(O)c(O)c1$"
      ],
      "metadata": {
        "id": "h6qDLD2b1N8t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10aifa4zfYmg"
      },
      "outputs": [],
      "source": [
        "cd /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, in order to train our Sequence to Sequence RNN, we have to append 'G' (Go character to mark the starting of SMILES string at the extreme left most of the string and '\\n' to mark the end of SMILES string at the extreme right most of the string. "
      ],
      "metadata": {
        "id": "0zr2nbRPUk6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the below code, fetching all the SMILES strings of drug molecules and appending 'G' character at the left most of the string and writing the appended strings with 'G' at the left most side, to the new file. "
      ],
      "metadata": {
        "id": "oSOgy151VqQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "jeuw4x6Bx61l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "\n",
        "completed_lines_hash = set()\n",
        "\n",
        "completed_lines_hash = set()\n",
        "\n",
        "#Save processed data to SMILES.txt\n",
        "new = open(\"smiles.txt\", \"w\")\n",
        "\n",
        "#Read in data file line by line\n",
        "for line in open(\"data.txt\", \"r\"):\n",
        "  \n",
        "    #Ensure all smiles in original data file are unique\n",
        "    hashValue = hashlib.md5(line.rstrip().encode('utf-8')).hexdigest()\n",
        "  \n",
        "    if hashValue not in completed_lines_hash:\n",
        "        completed_lines_hash.add(hashValue)\n",
        "        \n",
        "        #Ensure all SMILES are between 35 and 75 characters in length\n",
        "        if 34 < len(line) < 75:\n",
        "            #Add start token\n",
        "            line = line.rjust(len(line)+1, \"G\")\n",
        "\n",
        "            #Copy over SMILES satisfying requirements\n",
        "            new.write(line)\n",
        "    \n",
        "#Close files\n",
        "new.close()"
      ],
      "metadata": {
        "id": "I9pB0zTBupjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read in processed data file\n",
        "data = open(\"smiles.txt\", \"r\").read()\n",
        "\n",
        "#Create a list of the unique characters in the dataset\n",
        "chars = list(set(data))\n",
        "\n",
        "#Get size (in characters) of dataset\n",
        "data_size = len(data) \n",
        "\n",
        "#Get number of unique characters in dataset\n",
        "vocab_size = len(chars)\n",
        "\n",
        "#Print dataset properties\n",
        "print(\"Vocab size: \" + str(vocab_size))\n",
        "print(\"Data size: \" + str(data_size))\n",
        "print(\"Characters in data: \" + str(chars))"
      ],
      "metadata": {
        "id": "iJxFq0QLw5g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If seen carefully above then '\\n' is already included in the characters in processed data without even including explicitly by us. "
      ],
      "metadata": {
        "id": "jLf77HfRZCor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "#Create array from characters in the dataset\n",
        "values = np.array(chars)\n",
        "print(\"Array of unique characters:\")\n",
        "print(values)\n",
        "\n",
        "#Create unique, numerical labels for each character between 0 and n-1, where n is the number of unique characters\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values)\n",
        "print(\"Array of labels for each character:\")\n",
        "print(integer_encoded)\n",
        "\n",
        "#Encode characters into a one-hot encoding, resulting in an array of size [num unique chars, num unique chars]\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(\"Array of one-hot encoded characters:\")\n",
        "print(onehot_encoded)\n",
        "print(\"Size of array of one-hot encoded characters: \" + str(onehot_encoded.shape))"
      ],
      "metadata": {
        "id": "VsqeEBK1YapF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read in processed data file\n",
        "data = open(\"smiles.txt\", \"r\").read()\n",
        "#Create a list of the dataset\n",
        "datalist = list(data)\n",
        "#Create an array of the dataset\n",
        "dataarray = np.array(datalist)\n",
        "#Fit one-hot encoding to dataarray\n",
        "dataarray = dataarray.reshape(len(dataarray), 1)\n",
        "OHESMILES = onehot_encoder.fit_transform(dataarray).astype(int)\n",
        "print(\"Size of one-hot encoded array of data: \" + str(OHESMILES.shape))\n",
        "print(\"One-hot encoded array of data:\")\n",
        "print(OHESMILES)"
      ],
      "metadata": {
        "id": "L8TldPx3bS7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save OHESMILES as a (compressed) file\n",
        "np.savez_compressed(\"ohesmiles.npz\", OHESMILES)"
      ],
      "metadata": {
        "id": "DgGTqeaHbxyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create integer SMILES data\n",
        "INTSMILES = [np.where(r==1)[0][0] for r in OHESMILES]"
      ],
      "metadata": {
        "id": "35hjlNQBc3iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save INTSMILES as a (compressed) file\n",
        "np.savez_compressed(\"intsmiles.npz\", INTSMILES)"
      ],
      "metadata": {
        "id": "pIGkJIECdSsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save array with SMILES character, integer encoding, and one hot encoding (vocabulary)\n",
        "values = np.reshape(values, (np.shape(values)[0], 1))\n",
        "vocab = np.concatenate((values, integer_encoded.astype(object)), axis = 1)\n",
        "vocab = vocab[vocab[:,1].argsort()]\n",
        "vocabvalues = np.reshape(vocab[:,1], (-1,1))\n",
        "vocabohe = onehot_encoder.fit_transform(vocabvalues)\n",
        "vocabencodings = np.concatenate((vocab, vocabohe.astype(object)), axis = 1)\n",
        "print(np.shape(vocabencodings))\n",
        "\n",
        "np.save(\"vocab.npy\", vocabencodings)"
      ],
      "metadata": {
        "id": "oX3hc9-ydkVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocabencodings)"
      ],
      "metadata": {
        "id": "xA6y6Posd1Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load SMILES data as integer labels and as one-hot encoding\n",
        "data = np.load(\"ohesmiles.npz\")\n",
        "data = data[\"arr_0\"]\n",
        "\n",
        "intdata = np.load(\"intsmiles.npz\")\n",
        "intdata = intdata[\"arr_0\"]"
      ],
      "metadata": {
        "id": "SDNHgQ3feAfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "lnbem61qmT9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intdata.shape"
      ],
      "metadata": {
        "id": "AFa2QvximX8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.reshape(data.shape[0],1,data.shape[1])"
      ],
      "metadata": {
        "id": "A76fFotHm0YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "9-OzKk1eMLyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_examples_generator(sequence_length,batch_idx,batch_size):\n",
        "\n",
        "  #Now, we will generate a batch of SMILES string representations of drug molecules as input and one time step shifted to the right representations as\n",
        "  #training labels as our Generative model is RNN based therefore, the RNN variant is going to be Sequence to Sequence and hence requiring sequences as inputs\n",
        "  #as well as labels\n",
        "\n",
        "  training_features = data[int((sequence_length * batch_idx)) : int((sequence_length * batch_idx) + (sequence_length * batch_size)),:,:]\n",
        "  training_labels = intdata[int((sequence_length * batch_idx)) : int((sequence_length * batch_idx) + (sequence_length * batch_size))]\n",
        "\n",
        "  training_features_batch = np.zeros(((sequence_length-1) * batch_size, 1, data.shape[2]))\n",
        "  training_labels_batch = np.zeros(((sequence_length-1) * batch_size))\n",
        "\n",
        "  training_features_idx = 0\n",
        "  training_labels_idx = 0\n",
        "\n",
        "  for char_idx in range(sequence_length * batch_size - 1):\n",
        "\n",
        "    if char_idx % sequence_length != (sequence_length - 1):\n",
        "\n",
        "      #Training Features batch (does not include last character of SMILES string representation of each drug molecule in the batch)\n",
        "      training_features_batch[training_features_idx,:,:] = training_features[char_idx,:,:]\n",
        "      training_features_idx += 1\n",
        "\n",
        "      if char_idx % sequence_length != 0:\n",
        "\n",
        "        #Training Labels batch (does not include first character of SMILES string representation of each drug molecule in the batch)\n",
        "        training_labels_batch[training_labels_idx] = training_labels[char_idx]\n",
        "        training_labels_idx += 1\n",
        "\n",
        "  return training_features_batch,training_labels_batch"
      ],
      "metadata": {
        "id": "zKwxUkMUf03u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import Input\n",
        "from keras.layers import LSTM \n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential"
      ],
      "metadata": {
        "id": "LHD8fXut3vwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generative_seq2seq_model(maximum_sequence_length):\n",
        "\n",
        "  gen_rnn_model = Sequential()\n",
        "\n",
        "  gen_rnn_model.add(Input(shape=(maximum_sequence_length,)))\n",
        "  gen_rnn_model.add(LSTM(units=1024,dropout=0.2,return_sequences=True))\n",
        "  gen_rnn_model.add(LSTM(units=1024,dropout=0.2,return_sequences=True))\n",
        "  gen_rnn_model.add(LSTM(units=1024,dropout=0.2,return_sequences=True))\n",
        "  gen_rnn_model.add(Dense(units=data.shape[1],activation=\"softmax\"))\n",
        "\n",
        "  gen_rnn_model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "\n",
        "  return gen_rnn_model"
      ],
      "metadata": {
        "id": "-rZfMJ2byJAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = generative_seq2seq_model(75)"
      ],
      "metadata": {
        "id": "teOb5NNh8lZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "CR8z58Rt80mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_features_batch,training_labels_batch = training_examples_generator(75,0,128)"
      ],
      "metadata": {
        "id": "r_OPXRE_HeMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_features_batch.shape"
      ],
      "metadata": {
        "id": "0bla1_ddJ2D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels_batch.shape"
      ],
      "metadata": {
        "id": "IHNxoc-YgEuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7xh9TWXThlPl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}